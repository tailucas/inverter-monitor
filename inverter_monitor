#!/usr/bin/env python
import logging
import logging.handlers

import binascii
import builtins
import boto3
import copy
import json
import libscrc
import os
import re
import requests
import simplejson as json
import threading
import time
import zmq

from base64 import b64decode, b64encode
from collections import OrderedDict
from botocore.exceptions import EndpointConnectionError as bcece, \
    ConnectionClosedError as bccce, \
    ConnectionError as bce
from boto3.dynamodb.conditions import Attr
from bs4 import BeautifulSoup
from configparser import ConfigParser
from datetime import datetime, timedelta
from dateutil import tz
from decimal import Decimal
from io import BytesIO
from msgpack.exceptions import UnpackException
from onepasswordconnectsdk.client import Client as CredsClient
from operator import itemgetter
from pathlib import Path
from pylru import lrucache
from requests.exceptions import ConnectionError
from sentry_sdk import capture_exception, last_event_id
from sentry_sdk.integrations.logging import ignore_logger
from simplejson.scanner import JSONDecodeError
from socket import gaierror
from zmq.error import ZMQError, ContextTerminated, Again

import os.path

# setup builtins used by pylib init
app_name = Path(__file__).stem
builtins.APP_NAME = app_name
builtins.SENTRY_EXTRAS = []
AWS_REGION = os.environ['AWS_DEFAULT_REGION']
class CredsConfig:
    sentry_dsn: f'opitem:"Sentry" opfield:{app_name}.dsn' = None # type: ignore
    cronitor_token: f'opitem:"cronitor" opfield:.password' = None # type: ignore
    influxdb_org: f'opitem:"InfluxDB" opfield:automation.org' = None # type: ignore
    influxdb_token: f'opitem:"InfluxDB" opfield:automation.token' = None # type: ignore
    aws_akid: f'opitem:"AWS" opfield:{AWS_REGION}.akid' = None # type: ignore
    aws_sak: f'opitem:"AWS" opfield:{AWS_REGION}.sak' = None # type: ignore
# instantiate class
builtins.creds_config = CredsConfig()

from lib import app_config, \
    creds, \
    device_name, \
    device_name_base, \
    log, \
    log_handler

from lib.datetime import is_list, \
    make_timestamp, \
    make_unix_timestamp, \
    parse_datetime, \
    ISO_DATE_FORMAT
from lib.aws.metrics import post_count_metric
from lib.leader import Leader
from lib.process import SignalHandler, exec_cmd_log
from lib.rabbit import MQConnection, ZMQListener
from lib import threads
from lib.threads import thread_nanny, die
from lib.app import AppThread, ZmqRelay
from lib.zmq import zmq_term, Closable
from lib.handler import exception_handler

from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import ASYNCHRONOUS


URL_WORKER_APP = 'inproc://app-worker'

SAMPLE_INTERVAL_SECONDS = 60
ERROR_RETRY_INTERVAL_SECONDS = 60/6
ERROR_RETRY_ATTEMPTS = 5

FIELD_MAPPINGS = None


def twosComplement_hex(hexval):
    bits = 16
    val = int(hexval, bits)
    if val & (1 << (bits-1)):
        val -= 1 << bits
    return val


class LoggerReader(AppThread, Closable):

    def __init__(self):
        AppThread.__init__(self, name=self.__class__.__name__)
        Closable.__init__(self)

        self.processor = self.get_socket(zmq.PUSH)

    # noinspection PyBroadException
    def run(self):
        self.processor.connect(URL_WORKER_APP)
        with exception_handler(closable=self):
            while not threads.shutting_down:
                operation_start_time = time.time()
                self.processor.send_pyobj({"foo": "bar"})
                operation_end_time = time.time()
                # stop for the remainder of the sampling interval
                threads.interruptable_sleep.wait(SAMPLE_INTERVAL_SECONDS - (operation_end_time-operation_start_time))


class EventProcessor(AppThread, Closable):

    def __init__(self):
        AppThread.__init__(self, name=self.__class__.__name__)
        Closable.__init__(self, connect_url=URL_WORKER_APP)

        self.influxdb_url = app_config.get('influxdb', 'url')
        self.influxdb_bucket = app_config.get('influxdb', 'bucket')

        self.influxdb = None
        self.influxdb_rw = None
        self.influxdb_ro = None

    def _influxdb_write(self, field_name, field_value):
        try:
            self.influxdb_rw.write(
                bucket=self.influxdb_bucket,
                record=Point("inverter").tag("application", app_name).tag("device", device_name_base).field(field_name, field_value))
        except Exception:
            log.warning(f'Unable to post to InfluxDB.', exc_info=True)

    # noinspection PyBroadException
    def run(self):
        # influx DB
        self.influxdb = InfluxDBClient(
            url=self.influxdb_url,
            token=creds.influxdb_token,
            org=creds.influxdb_org)
        self.influxdb_rw = self.influxdb.write_api(write_options=ASYNCHRONOUS)
        self.influxdb_ro = self.influxdb.query_api()
        with exception_handler(closable=self, and_raise=False, shutdown_on_error=True):
            while not threads.shutting_down:
                event = self.socket.recv_pyobj()
                log.debug(event)
                if isinstance(event, dict):
                    log.info(event)


if __name__ == "__main__":
    log.setLevel(logging.INFO)
    # load basic configuration
    field_mappings_file = ''.join([os.path.join(APP_PATH, 'config', 'field_mappings.txt')]) # type: ignore
    with open(field_mappings_file) as mappings_file:
        try:
            FIELD_MAPPINGS = json.loads(mappings_file.read())
            log.info(f'Loaded {len(FIELD_MAPPINGS)} field mappings from {field_mappings_file}')
        except JSONDecodeError as e:
            log.exception(f'Error loading {field_mappings_file}.')
            raise e
    # ensure proper signal handling; must be main thread
    signal_handler = SignalHandler()
    event_processor = EventProcessor()
    logger_reader = LoggerReader()
    nanny = threading.Thread(
        name='nanny',
        target=thread_nanny,
        args=(signal_handler,))
    nanny.setDaemon(True)
    # startup completed
    # back to INFO logging
    log.setLevel(logging.INFO)
    try:
        log.info('Starting application threads...')
        event_processor.start()
        logger_reader.start()
        # start thread nanny
        nanny.start()
        log.info('Startup complete.')
        # hang around until something goes wrong
        threads.interruptable_sleep.wait()
        raise RuntimeWarning("Shutting down...")
    except(KeyboardInterrupt, RuntimeWarning, ContextTerminated) as e:
        log.warning(str(e))
        threads.shutting_down = True
        threads.interruptable_sleep.set()
    finally:
        zmq_term()
    log.info('Shutdown complete.')